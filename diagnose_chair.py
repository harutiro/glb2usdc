#!/usr/bin/env python3
"""
Chair GLBãƒ•ã‚¡ã‚¤ãƒ«ã®è©³ç´°è¨ºæ–­ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
ãƒ¡ãƒƒã‚·ãƒ¥ãƒ‡ãƒ¼ã‚¿ã€é ‚ç‚¹ã€ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’åˆ†æã—ã¦ç•°å¸¸ã‚’æ¤œå‡º
"""

import sys
import numpy as np
from pathlib import Path

try:
    from gltflib import GLTF
except ImportError:
    print("gltflib not found. Please install it with: pip install gltflib")
    sys.exit(1)

def analyze_glb(file_path):
    """GLBãƒ•ã‚¡ã‚¤ãƒ«ã‚’è©³ç´°åˆ†æ"""
    print(f"=== Analyzing GLB: {file_path} ===")
    
    # GLTFã‚’ãƒ­ãƒ¼ãƒ‰
    gltf = GLTF.load(str(file_path))
    
    # ãƒã‚¤ãƒŠãƒªãƒ‡ãƒ¼ã‚¿ã‚’ãƒ­ãƒ¼ãƒ‰
    with open(file_path, 'rb') as f:
        # GLBãƒ˜ãƒƒãƒ€ãƒ¼ï¼ˆ12ãƒã‚¤ãƒˆï¼‰ã‚’ã‚¹ã‚­ãƒƒãƒ—
        f.seek(12)
        
        # JSONãƒãƒ£ãƒ³ã‚¯ã®ã‚µã‚¤ã‚ºã‚’èª­ã‚€
        json_chunk_length = int.from_bytes(f.read(4), 'little')
        f.seek(20)  # JSONãƒãƒ£ãƒ³ã‚¯ã‚¿ã‚¤ãƒ—ã®å¾Œ
        
        # JSONãƒ‡ãƒ¼ã‚¿ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¦ã€BINãƒãƒ£ãƒ³ã‚¯ã«ç§»å‹•
        f.seek(20 + json_chunk_length)
        
        # BINãƒãƒ£ãƒ³ã‚¯ã®ã‚µã‚¤ã‚ºã‚’èª­ã‚€
        bin_chunk_length = int.from_bytes(f.read(4), 'little')
        bin_chunk_type = f.read(4)  # should be b'BIN\x00'
        
        # ãƒã‚¤ãƒŠãƒªãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚€
        buffer_data = f.read(bin_chunk_length)
        print(f"Binary buffer size: {len(buffer_data):,} bytes")
    
    # ã‚·ãƒ¼ãƒ³æƒ…å ±
    print(f"\nScenes: {len(gltf.model.scenes)}")
    if gltf.model.scenes:
        scene = gltf.model.scenes[0]
        if hasattr(scene, 'nodes'):
            print(f"Root nodes in scene: {len(scene.nodes)}")
    
    # ãƒãƒ¼ãƒ‰æƒ…å ±
    print(f"\nTotal nodes: {len(gltf.model.nodes)}")
    mesh_nodes = []
    for i, node in enumerate(gltf.model.nodes):
        name = getattr(node, 'name', f'node_{i}')
        has_mesh = hasattr(node, 'mesh') and node.mesh is not None
        has_children = hasattr(node, 'children') and node.children
        child_count = len(node.children) if has_children else 0
        
        print(f"  Node {i:2d}: {name:<25} mesh={has_mesh:<5} children={child_count}")
        
        if has_mesh:
            mesh_nodes.append((i, name, node.mesh))
    
    print(f"\nNodes with meshes: {len(mesh_nodes)}")
    
    # ãƒ¡ãƒƒã‚·ãƒ¥æƒ…å ±è©³ç´°
    print(f"\nMesh analysis:")
    print(f"Total meshes: {len(gltf.model.meshes)}")
    
    total_vertices = 0
    total_triangles = 0
    
    for mesh_idx, mesh in enumerate(gltf.model.meshes):
        mesh_name = getattr(mesh, 'name', f'mesh_{mesh_idx}')
        print(f"\n  Mesh {mesh_idx}: {mesh_name}")
        print(f"    Primitives: {len(mesh.primitives)}")
        
        for prim_idx, primitive in enumerate(mesh.primitives):
            print(f"    Primitive {prim_idx}:")
            
            # é ‚ç‚¹ä½ç½®ã‚’å–å¾—
            if hasattr(primitive.attributes, 'POSITION'):
                position_accessor_idx = primitive.attributes.POSITION
                position_accessor = gltf.model.accessors[position_accessor_idx]
                vertex_count = position_accessor.count
                total_vertices += vertex_count
                print(f"      Vertices: {vertex_count}")
                
                # å®Ÿéš›ã®é ‚ç‚¹ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚“ã§åˆ†æ
                positions = get_buffer_data(gltf, buffer_data, position_accessor_idx)
                if positions is not None:
                    min_pos = np.min(positions, axis=0)
                    max_pos = np.max(positions, axis=0)
                    center = np.mean(positions, axis=0)
                    print(f"      Position range: [{min_pos[0]:.3f}, {min_pos[1]:.3f}, {min_pos[2]:.3f}] to [{max_pos[0]:.3f}, {max_pos[1]:.3f}, {max_pos[2]:.3f}]")
                    print(f"      Center: [{center[0]:.3f}, {center[1]:.3f}, {center[2]:.3f}]")
                    
                    # ç•°å¸¸ãªå€¤ã‚’ãƒã‚§ãƒƒã‚¯
                    if np.any(np.abs(positions) > 1000):
                        print(f"      âš ï¸  WARNING: Extreme vertex positions detected!")
                        extreme_indices = np.where(np.abs(positions) > 1000)
                        print(f"         Extreme vertices: {len(extreme_indices[0])} vertices")
                    
                    # NaNã‚„Infã‚’ãƒã‚§ãƒƒã‚¯
                    if np.any(np.isnan(positions)) or np.any(np.isinf(positions)):
                        print(f"      âŒ ERROR: NaN or Inf values in vertex positions!")
                    
                    # é‡è¤‡é ‚ç‚¹ã‚’ãƒã‚§ãƒƒã‚¯
                    unique_positions = np.unique(positions.view(np.void), return_counts=True)
                    duplicate_count = np.sum(unique_positions[1] > 1)
                    if duplicate_count > 0:
                        print(f"      ğŸ”„ Duplicate vertices: {duplicate_count}")
            
            # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’å–å¾—
            if hasattr(primitive, 'indices') and primitive.indices is not None:
                indices_accessor_idx = primitive.indices
                indices_accessor = gltf.model.accessors[indices_accessor_idx]
                indices_count = indices_accessor.count
                triangle_count = indices_count // 3
                total_triangles += triangle_count
                print(f"      Indices: {indices_count} ({triangle_count} triangles)")
                
                # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚“ã§åˆ†æ
                indices = get_buffer_data(gltf, buffer_data, indices_accessor_idx)
                if indices is not None:
                    indices = indices.flatten()
                    max_vertex_idx = vertex_count - 1 if 'vertex_count' in locals() else 0
                    
                    # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ç¯„å›²ãƒã‚§ãƒƒã‚¯
                    if np.any(indices > max_vertex_idx):
                        invalid_indices = np.sum(indices > max_vertex_idx)
                        print(f"      âŒ ERROR: {invalid_indices} indices exceed vertex count!")
                        print(f"         Max valid index: {max_vertex_idx}, Found indices up to: {np.max(indices)}")
                    
                    # é‡è¤‡ä¸‰è§’å½¢ã‚’ãƒã‚§ãƒƒã‚¯
                    triangles = indices.reshape(-1, 3)
                    sorted_triangles = np.sort(triangles, axis=1)
                    unique_triangles = np.unique(sorted_triangles.view(np.void), return_counts=True)
                    duplicate_triangles = np.sum(unique_triangles[1] > 1)
                    if duplicate_triangles > 0:
                        print(f"      ğŸ”„ Duplicate triangles: {duplicate_triangles}")
                    
                    # é€€åŒ–ä¸‰è§’å½¢ã‚’ãƒã‚§ãƒƒã‚¯ï¼ˆåŒã˜é ‚ç‚¹ã‚’æŒã¤ä¸‰è§’å½¢ï¼‰
                    degenerate_count = 0
                    for triangle in triangles:
                        if len(np.unique(triangle)) < 3:
                            degenerate_count += 1
                    if degenerate_count > 0:
                        print(f"      âš ï¸  Degenerate triangles: {degenerate_count}")
            
            # ãƒãƒ†ãƒªã‚¢ãƒ«æƒ…å ±
            if hasattr(primitive, 'material') and primitive.material is not None:
                material_idx = primitive.material
                if material_idx < len(gltf.model.materials):
                    material = gltf.model.materials[material_idx]
                    material_name = getattr(material, 'name', f'material_{material_idx}')
                    print(f"      Material: {material_idx} ({material_name})")
    
    print(f"\n=== SUMMARY ===")
    print(f"Total vertices across all meshes: {total_vertices:,}")
    print(f"Total triangles across all meshes: {total_triangles:,}")
    print(f"Average vertices per mesh: {total_vertices // len(gltf.model.meshes) if gltf.model.meshes else 0}")

def get_buffer_data(gltf, buffer_data, accessor_idx):
    """ã‚¢ã‚¯ã‚»ã‚µãƒ¼ã‹ã‚‰ãƒãƒƒãƒ•ã‚¡ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—"""
    try:
        if accessor_idx is None or not gltf.model.accessors or accessor_idx >= len(gltf.model.accessors):
            return None
            
        accessor = gltf.model.accessors[accessor_idx]
        buffer_view = gltf.model.bufferViews[accessor.bufferView]
        
        # ã‚ªãƒ•ã‚»ãƒƒãƒˆã¨ã‚µã‚¤ã‚ºã‚’è¨ˆç®—
        byte_offset = buffer_view.byteOffset or 0
        if hasattr(accessor, 'byteOffset'):
            byte_offset += accessor.byteOffset or 0
        
        # ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã‚¿ã‚¤ãƒ—ã«åŸºã¥ã„ã¦ãƒ‡ãƒ¼ã‚¿å‹ã‚’æ±ºå®š
        dtype_map = {
            5120: np.int8,    # BYTE
            5121: np.uint8,   # UNSIGNED_BYTE
            5122: np.int16,   # SHORT
            5123: np.uint16,  # UNSIGNED_SHORT
            5125: np.uint32,  # UNSIGNED_INT
            5126: np.float32, # FLOAT
        }
        
        dtype = dtype_map.get(accessor.componentType, np.float32)
        
        # ã‚¿ã‚¤ãƒ—ã«åŸºã¥ã„ã¦è¦ç´ æ•°ã‚’æ±ºå®š
        type_sizes = {
            'SCALAR': 1,
            'VEC2': 2,
            'VEC3': 3,
            'VEC4': 4,
            'MAT2': 4,
            'MAT3': 9,
            'MAT4': 16
        }
        
        component_count = type_sizes.get(accessor.type, 1)
        element_size = np.dtype(dtype).itemsize * component_count
        
        # ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿å–ã‚Š
        data = []
        for i in range(accessor.count):
            start = byte_offset + i * element_size
            end = start + element_size
            element_data = buffer_data[start:end]
            element = np.frombuffer(element_data, dtype=dtype, count=component_count)
            data.append(element)
        
        return np.array(data)
    except Exception as e:
        print(f"Error reading buffer data: {e}")
        return None

if __name__ == '__main__':
    import sys
    if len(sys.argv) > 1:
        file_path = Path(sys.argv[1])
    else:
        file_path = Path('chair.glb')
    
    if file_path.exists():
        analyze_glb(file_path)
    else:
        print(f"File not found: {file_path}")